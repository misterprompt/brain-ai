# -*- coding: utf-8 -*-
"""
ğŸ” DEEP SEARCH v9 - Recherche Approfondie AcadÃ©mique
=====================================================
Version complÃ¨tement refondÃ©e avec:
- Orchestration multi-API parallÃ¨le
- Scoring transparent avec sous-scores
- SynthÃ¨se stricte avec citations inline
- Tableau acadÃ©mique
- FAQ sourcÃ©e
- Badge requires_human_review

CritÃ¨res d'acceptation:
- DiversitÃ© des providers > 10 par requÃªte
- Snippets visibles pour chaque affirmation
- Tableau acadÃ©mique prÃ©sent et complet
- Score expliquÃ© avec sous-scores
"""

import asyncio
import json
import logging
import time
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional, AsyncGenerator
from urllib.parse import quote

from fastapi import APIRouter, Query
from fastapi.responses import StreamingResponse

# Imports internes
from services.multi_api_orchestrator import (
    orchestrate_search,
    calculate_confidence_v7,
    cluster_by_theme
)
from services.api_registry import Category, get_categories_summary
from services.ai_router import ai_router
from services.content_filter import filter_search_results

logger = logging.getLogger(__name__)
router = APIRouter()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONSTANTES & CRITÃˆRES D'ACCEPTATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MIN_PROVIDERS_TARGET = 12  # Objectif: â‰¥12 providers distincts
MIN_PEER_REVIEW = 6        # â‰¥6 peer-review/Ã©diteurs
MIN_OFFICIAL = 4           # â‰¥4 sources officielles
MAX_SOURCES_FOR_SYNTHESIS = 30
MAX_BOOKS_VISIBLE = 2      # Livres masquÃ©s par dÃ©faut (max 2)

# Quotas par domaine dÃ©tectÃ©
CATEGORY_QUOTAS = {
    "health": {"academic": 5, "official": 5, "books": 2},
    "ai_ml": {"academic": 5, "official": 2, "books": 1},
    "general": {"academic": 4, "official": 3, "books": 2}
}

# Seuils de revue humaine
HUMAN_REVIEW_THRESHOLDS = {
    "min_score": 70,
    "min_providers": 10,
    "min_peer_review_ratio": 0.30,
    "max_preprint_ratio": 0.50
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DÃ‰TECTION DE LANGUE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def detect_language(query: str) -> str:
    """DÃ©tecte la langue de la requÃªte."""
    q_lower = query.lower()
    
    # FranÃ§ais
    french_markers = ["qu'est-ce", "comment", "pourquoi", "oÃ¹", "quand", "quel", "est-ce", 
                      "le", "la", "les", "un", "une", "des", "du", "de", "Ã ", "en"]
    french_score = sum(1 for m in french_markers if m in q_lower)
    
    # Anglais
    english_markers = ["what", "how", "why", "where", "when", "which", "the", "is", "are", "of"]
    english_score = sum(1 for m in english_markers if m in q_lower)
    
    if french_score > english_score:
        return "fr"
    elif english_score > french_score:
        return "en"
    
    # HÃ©breu
    if any('\u0590' <= c <= '\u05FF' for c in query):
        return "he"
    
    # Arabe
    if any('\u0600' <= c <= '\u06FF' for c in query):
        return "ar"
    
    return "fr"  # DÃ©faut


def detect_domain(query: str) -> str:
    """DÃ©tecte le domaine de la requÃªte pour appliquer les quotas."""
    q_lower = query.lower()
    
    # Termes IA/ML
    ai_terms = ["ai", "ml", "machine learning", "deep learning", "neural", "model", 
                "gym", "openai", "reinforcement", "rl", "transformer", "gpt", "llm",
                "robot", "algorithm", "dataset", "training", "inference", "pytorch",
                "tensorflow", "environment", "agent", "policy"]
    ai_score = sum(1 for t in ai_terms if t in q_lower)
    
    # Termes santÃ©
    health_terms = ["health", "santÃ©", "mÃ©dical", "medical", "disease", "maladie",
                    "treatment", "traitement", "symptom", "symptÃ´me", "diagnosis",
                    "diagnostic", "patient", "clinical", "clinique", "therapy",
                    "thÃ©rapie", "drug", "mÃ©dicament", "vaccine", "vaccin", "cancer",
                    "vih", "hiv", "sida", "aids", "diabetes", "diabÃ¨te", "exercise",
                    "fitness", "gym", "workout", "muscle", "nutrition"]
    health_score = sum(1 for t in health_terms if t in q_lower)
    
    # DÃ©cision
    if ai_score > health_score and ai_score >= 2:
        return "ai_ml"
    elif health_score > ai_score and health_score >= 2:
        return "health"
    
    return "general"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GÃ‰NÃ‰RATION DU TABLEAU ACADÃ‰MIQUE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_academic_table(sources: List[Dict]) -> str:
    """GÃ©nÃ¨re un tableau acadÃ©mique Markdown."""
    if not sources:
        return "| Aucune source acadÃ©mique disponible |"
    
    # En-tÃªte
    table = "| # | Auteur/Source | AnnÃ©e | Type | Provider | URL/DOI |\n"
    table += "|---|---------------|-------|------|----------|----------|\n"
    
    for i, source in enumerate(sources[:15]):  # Max 15 lignes
        idx = i + 1
        
        # Extraire les mÃ©tadonnÃ©es
        metadata = source.get("metadata", {})
        authors = metadata.get("authors", source.get("provider", "N/A"))
        year = metadata.get("year") or source.get("timestamp", "")[:4]
        source_type = source.get("source_type", "N/A")
        provider = source.get("provider", "N/A")
        url = source.get("url", "")
        
        # Tronquer l'URL pour l'affichage
        doi = metadata.get("doi", "")
        if doi:
            display_url = f"[DOI]({url})"
        elif url:
            display_url = f"[Lien]({url[:60]}...)" if len(url) > 60 else f"[Lien]({url})"
        else:
            display_url = "N/A"
        
        # Tronquer les auteurs
        if len(authors) > 30:
            authors = authors[:27] + "..."
        
        row = f"| {idx} | {authors} | {year} | {source_type} | {provider} | {display_url} |\n"
        table += row
    
    return table


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GÃ‰NÃ‰RATION DES PREUVES PAR THÃˆME (avec snippets visibles)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_evidence_by_theme(clusters: Dict[str, List[Dict]]) -> str:
    """GÃ©nÃ¨re les preuves organisÃ©es par thÃ¨me avec snippets visibles."""
    output = []
    
    for theme, sources in clusters.items():
        if not sources:
            continue
        
        output.append(f"\n### {theme} ({len(sources)} sources)\n")
        
        for i, source in enumerate(sources[:5]):  # Max 5 par thÃ¨me
            idx = i + 1
            title = source.get("title", "Sans titre")[:80]
            snippet = source.get("snippet", "")[:300]
            provider = source.get("provider", "N/A")
            url = source.get("url", "")
            confidence = source.get("raw_confidence", 0)
            
            output.append(f"**[{idx}] {title}** ({provider})")
            output.append(f"> {snippet}")
            if url:
                output.append(f"ğŸ”— [Source]({url}) | Confiance: {int(confidence*100)}%")
            output.append("")
    
    return "\n".join(output)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROMPT DE SYNTHÃˆSE ACADÃ‰MIQUE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_synthesis_prompt(query: str, sources: List[Dict]) -> str:
    """Construit le prompt de synthÃ¨se VULGARISÃ‰E et accessible au grand public."""
    
    # NumÃ©roter les sources avec contenu pertinent uniquement (pas de mÃ©tadonnÃ©es brutes)
    numbered_sources = []
    for i, source in enumerate(sources[:MAX_SOURCES_FOR_SYNTHESIS]):
        idx = i + 1
        title = source.get("title", "")[:100]
        snippet = source.get("snippet", "")[:500]
        provider = source.get("provider", "N/A")
        
        # Nettoyer le snippet des mÃ©tadonnÃ©es techniques
        clean_snippet = snippet
        for noise in ["DOI:", "Auteurs:", "Authors:", "Ã‰diteur:", "Publisher:", "Article scientifique."]:
            clean_snippet = clean_snippet.replace(noise, "")
        clean_snippet = clean_snippet.strip()
        
        if clean_snippet:  # Ne garder que les sources avec du contenu utile
            numbered_sources.append(
                f"[{idx}] {provider}\n"
                f"   Titre: {title}\n"
                f"   Contenu: \"{clean_snippet}\""
            )
    
    sources_text = "\n\n".join(numbered_sources)
    
    prompt = f"""RECHERCHE APPROFONDIE: "{query}"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“š SOURCES DISPONIBLES ({len(numbered_sources)} rÃ©fÃ©rences)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{sources_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ MISSION: RAPPORT D'EXPERTISE EXHAUSTIF (DEEP REPORT)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Tu es un CHERCHEUR ANALYSTE travaillant sur un rapport officiel pour des professionnels.
TA MISSION est de synthÃ©tiser ces informations pour produire un document de rÃ©fÃ©rence COMPLET et DÃ‰TAILLÃ‰.

ğŸ“ CONSIGNES DE RÃ‰DACTION STRICTES:
1. EXHAUSTIVITÃ‰ MAXIMALE : Ne laisse aucun dÃ©tail technique pertinent de cÃ´tÃ©.
2. CITATIONS PRÃ‰CISES : Chaque affirmation doit Ãªtre sourcÃ©e avec [X].
3. STRUCTURE CANONIQUE OBLIGATOIRE :
   - ğŸ¢ TITRE ACADÃ‰MIQUE
   - ğŸ“Œ RÃ‰SUMÃ‰ EXÃ‰CUTIF (L'essentiel pour les dÃ©cideurs)
   - ğŸ“– CONTEXTE & DÃ‰FINITIONS (Historique, concepts)
   - ğŸ”¬ ANALYSE DÃ‰TAILLÃ‰E (CÅ“ur du rapport, sous-sections thÃ©matiques)
   - ğŸ“Š CHIFFRES CLÃ‰S & DONNÃ‰ES (Si disponibles)
   - âš–ï¸ DISCUSSION / CONTROVERSES (Limites, dÃ©bats)
   - ğŸ”® PERSPECTIVES (R&D, futur)
   - ğŸ’¡ CONCLUSION

4. TON : Expert, neutre, analytique. Pas de vulgarisation excessive.
5. LONGUEUR : VISEZ 3000 MOTS SI POSSIBLE. DÃ©veloppez chaque point.

âš ï¸ INSTRUCTION CRITIQUE : Ce rapport doit Ãªtre dense, riche et techniquement prÃ©cis. Ne rÃ©sumez pas, DÃ‰VELOPPEZ."""

    return prompt



def build_faq_prompt(query: str, sources: List[Dict]) -> str:
    """Construit le prompt pour une FAQ accessible et vulgarisÃ©e."""
    
    snippets = []
    for i, s in enumerate(sources[:10]):
        snippet = s.get('snippet', '')[:200]
        # Nettoyer les mÃ©tadonnÃ©es
        for noise in ["DOI:", "Auteurs:", "Authors:", "Ã‰diteur:", "Publisher:"]:
            snippet = snippet.replace(noise, "")
        if snippet.strip():
            snippets.append(f"[{i+1}] {snippet.strip()}")
    
    snippets_text = "\n".join(snippets)
    
    return f"""BasÃ© sur ces informations sur "{query}":
{snippets_text}

ğŸ¯ GÃ‰NÃˆRE 3 QUESTIONS/RÃ‰PONSES que se poserait un dÃ©butant curieux.

ğŸ“– RÃˆGLES:
- Questions SIMPLES que tout le monde peut comprendre
- RÃ©ponses CLAIRES sans jargon technique
- Si tu utilises un terme technique, EXPLIQUE-LE
- Ton conversationnel et engageant

FORMAT:

**â“ Q1:** [Question simple et naturelle]
**ğŸ’¬ R1:** [RÃ©ponse claire et accessible, 2-3 phrases max] [Source X]

**â“ Q2:** [Question que se poserait quelqu'un qui dÃ©couvre le sujet]
**ğŸ’¬ R2:** [RÃ©ponse pÃ©dagogique avec exemple si possible] [Source Y]

**â“ Q3:** [Question pratique "Ã  quoi Ã§a sert" ou "comment Ã§a marche"]
**ğŸ’¬ R3:** [RÃ©ponse concrÃ¨te avec application rÃ©elle] [Source Z]

âš ï¸ Ã‰VITE: "selon les chercheurs", "la littÃ©rature montre", termes acadÃ©miques."""

# ENDPOINT PRINCIPAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.get("/api/v6/deep-search")
async def deep_search(
    q: str = Query(..., min_length=2, description="La requÃªte de recherche"),
    lang: str = Query("fr", description="Langue de la rÃ©ponse (fr/en/auto)"),
    mode: str = Query("balanced", description="Mode: speed/balanced/deep")
):
    """
    Endpoint principal de Deep Search V9 (SSE Streaming).
    Orchestre la recherche multi-API, le clustering et la synthÃ¨se.
    """
    return StreamingResponse(
        deep_search_generator_v9(q, lang, mode),
        media_type="text/event-stream"
    )

@router.get("/api/deep-search/health")
async def health_check():
    return {"status": "healthy", "service": "deep-search-v9"}


async def deep_search_generator_v9(query: str, lang: str = "fr", mode: str = "balanced") -> AsyncGenerator[str, None]:
    """
    GÃ©nÃ©rateur SSE pour la Deep Search AcadÃ©mique v9.
    
    Ordre strict:
    1. Init
    2. Orchestration multi-API
    3. DÃ©duplication & Clustering
    4. Scoring transparent
    5. SynthÃ¨se stricte
    6. Tableau acadÃ©mique
    7. FAQ sourcÃ©e
    8. Preuves par thÃ¨me
    9. RÃ©fÃ©rences complÃ¨tes
    10. Fin
    """
    
    request_id = str(uuid.uuid4())[:8]
    start_time = time.time()
    
    # DÃ©tection langue
    if lang == "auto":
        lang = detect_language(query)
    
    # DÃ©tection domaine pour quotas
    domain = detect_domain(query)
    quotas = CATEGORY_QUOTAS.get(domain, CATEGORY_QUOTAS["general"])
    
    def sse(event_type: str, data: Any) -> str:
        return f"data: {json.dumps({'type': event_type, 'data': data}, ensure_ascii=False)}\n\n"
    
    try:
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 1: INIT
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        yield sse("init", {
            "request_id": request_id,
            "query": query,
            "lang": lang,
            "domain": domain,
            "quotas": quotas,
            "version": "v10-strict",
            "categories_available": get_categories_summary(),
            "acceptance_criteria": {
                "min_providers": MIN_PROVIDERS_TARGET,
                "min_peer_review": MIN_PEER_REVIEW,
                "min_official": MIN_OFFICIAL
            }
        })
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 1.5: IMAGES (pour faire patienter l'utilisateur)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        try:
            from services.smart_search_v7 import smart_search_v7
            images = await asyncio.wait_for(
                smart_search_v7.fetch_images(query, max_results=4),
                timeout=3.0
            )
            if images:
                yield sse("images", images)
        except Exception as img_err:
            logger.debug(f"Images fetch failed (non-critical): {img_err}")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 2: ORCHESTRATION MULTI-API
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        yield sse("stage", {"message": "ğŸš€ Interrogation de toutes les APIs..."})
        
        orchestration_result = await orchestrate_search(query, lang)
        
        sources = orchestration_result.get("sources", [])
        
        # ğŸ›¡ï¸ FILTRAGE CONTENU INAPPROPRIÃ‰
        sources = filter_search_results(sources)
        
        providers_consulted = orchestration_result.get("providers_consulted", [])
        stats = orchestration_result.get("stats", {})
        timings = orchestration_result.get("timings", {})
        by_category = orchestration_result.get("by_category", {})
        
        # Ã‰vÃ©nement orchestration complÃ¨te
        yield sse("orchestration_done", {
            "total_sources": len(sources),
            "providers_count": len(providers_consulted),
            "providers_list": providers_consulted,
            "by_source_type": stats.get("by_source_type", {}),
            "elapsed_ms": stats.get("total_time_ms", 0)
        })
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰VÃ‰NEMENT PIPELINE DEBUG - Transparence chaÃ®ne de traitement
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        pipeline_info = []
        for cat, cat_sources in by_category.items():
            pipeline_info.append({
                "category": cat,
                "sources_count": len(cat_sources),
                "status": "âœ…" if len(cat_sources) > 0 else "âŒ"
            })
        
        yield sse("pipeline_debug", {
            "message": "ğŸ“¡ DÃ©tails du pipeline de recherche",
            "categories_queried": list(by_category.keys()),
            "results_per_category": {k: len(v) for k, v in by_category.items()},
            "api_timings": timings,
            "total_raw_sources": sum(len(v) for v in by_category.values()),
            "total_after_dedup": len(sources),
            "dedup_removed": sum(len(v) for v in by_category.values()) - len(sources),
            "pipeline_steps": [
                {"step": "1. Fetch parallÃ¨le", "status": "âœ…", "details": f"{len(by_category)} catÃ©gories interrogÃ©es"},
                {"step": "2. Parsing/Normalisation", "status": "âœ…", "details": f"{sum(len(v) for v in by_category.values())} rÃ©sultats bruts"},
                {"step": "3. DÃ©duplication", "status": "âœ…", "details": f"{len(sources)} rÃ©sultats uniques"},
                {"step": "4. Clustering", "status": "pending", "details": "En cours..."}
            ]
        })
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 3: CLUSTERING THÃ‰MATIQUE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        yield sse("stage", {"message": "ğŸ“Š Organisation par thÃ¨me..."})
        
        clusters = cluster_by_theme(sources, query)
        
        yield sse("clusters", {
            "themes": list(clusters.keys()),
            "counts": {k: len(v) for k, v in clusters.items()}
        })
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 4B: RAPPORTS THÃ‰MATIQUES (Multi-Report)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        yield sse("stage", {"message": "ğŸ“ GÃ©nÃ©ration des rapports thÃ©matiques..."})
        
        thematic_reports = []
        top_themes = list(clusters.keys())[:3]  # Top 3 thÃ¨mes
        
        async def generate_theme_report(theme_name: str, theme_sources: List[Dict]) -> Dict:
            """GÃ©nÃ¨re un rapport focalisÃ© sur un thÃ¨me spÃ©cifique."""
            if not theme_sources:
                return {"theme": theme_name, "content": "Aucune source pour ce thÃ¨me.", "sources_count": 0}
            
            # Construire le contexte du thÃ¨me
            theme_context = "\n".join([
                f"[{i+1}] {s.get('title', '')}: {s.get('snippet', '')[:200]}"
                for i, s in enumerate(theme_sources[:10])
            ])
            
            theme_prompt = f"""THÃˆME: {theme_name}
            
Sources disponibles ({len(theme_sources)}):
{theme_context}

RÃ©dige une analyse focalisÃ©e sur ce thÃ¨me spÃ©cifique.
Structure:
## {theme_name}
- Points clÃ©s (3-5 bullets)
- Analyse dÃ©taillÃ©e (200 mots)
- Implications

Cite tes sources avec [X]. Sois prÃ©cis et factuel."""
            
            try:
                result = await ai_router.route(
                    prompt=theme_prompt,
                    system_prompt="Tu es un analyste expert. SynthÃ¨se thÃ©matique.",
                    preferred_provider="groq",
                    max_tokens=600
                )
                return {
                    "theme": theme_name,
                    "content": result.get("response", ""),
                    "sources_count": len(theme_sources)
                }
            except Exception as e:
                logger.error(f"Theme report error for {theme_name}: {e}")
                return {"theme": theme_name, "content": f"Erreur: {e}", "sources_count": 0}
        
        # GÃ©nÃ©ration parallÃ¨le des 3 rapports thÃ©matiques
        if top_themes:
            theme_tasks = [
                generate_theme_report(theme, clusters.get(theme, []))
                for theme in top_themes
            ]
            thematic_reports = await asyncio.gather(*theme_tasks)
            
            yield sse("thematic_reports", {"reports": thematic_reports})
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 5: SCORING TRANSPARENT
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        yield sse("stage", {"message": "ğŸ“ˆ Calcul de confiance..."})
        
        confidence = calculate_confidence_v7(sources, query)
        
        yield sse("confidence", confidence)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 5: SYNTHÃˆSE VULGARISÃ‰E (IA)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        yield sse("stage", {"message": "ğŸ¤– RÃ©daction d'une synthÃ¨se accessible..."})
        
        synthesis_text = ""
        try:
            synthesis_prompt = build_synthesis_prompt(query, sources)
            
            synthesis_result = await ai_router.route(
                prompt=synthesis_prompt,
                system_prompt="Tu es un chercheur expert. Redige un rapport detaille.",
                preferred_provider="openrouter",  # Utilise DeepSeek pour la longueur et la qualitÃ©
                max_tokens=4000
            )

            
            synthesis_text = synthesis_result.get("response", "")
            
            yield sse("synthesis", {"text": synthesis_text})
            
        except Exception as e:
            logger.error(f"Synthesis error: {e}")
            yield sse("synthesis", {"text": f"âš ï¸ Erreur lors de la synthÃ¨se: {e}", "error": True})
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 6: TABLEAU ACADÃ‰MIQUE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        yield sse("stage", {"message": "ğŸ“‹ GÃ©nÃ©ration du tableau acadÃ©mique..."})
        
        academic_table = generate_academic_table(sources)
        
        yield sse("academic_table", {"markdown": academic_table})
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 7: FAQ SOURCÃ‰E
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        yield sse("stage", {"message": "â“ GÃ©nÃ©ration de la FAQ..."})
        
        try:
            faq_prompt = build_faq_prompt(query, sources)
            
            faq_result = await ai_router.route(
                prompt=faq_prompt,
                system_prompt="FAQ concise et sourcÃ©e.",
                preferred_provider="mistral",
                max_tokens=500
            )
            
            yield sse("faq", {"text": faq_result.get("response", "")})
            
        except Exception as e:
            logger.error(f"FAQ error: {e}")
            yield sse("faq", {"text": "", "error": True})
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 8: PREUVES PAR THÃˆME (avec snippets)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        evidence_markdown = generate_evidence_by_theme(clusters)
        
        yield sse("evidence_by_theme", {
            "markdown": evidence_markdown,
            "clusters": {
                theme: [
                    {
                        "id": s["id"],
                        "title": s["title"],
                        "snippet": s["snippet"],
                        "provider": s["provider"],
                        "source_type": s["source_type"],
                        "url": s["url"],
                        "raw_confidence": s.get("raw_confidence", 0)
                    }
                    for s in sources_list
                ]
                for theme, sources_list in clusters.items()
            }
        })
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 9: RÃ‰FÃ‰RENCES COMPLÃˆTES
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        yield sse("references", {
            "sources": [
                {
                    "id": s["id"],
                    "index": i + 1,
                    "title": s["title"],
                    "url": s["url"],
                    "provider": s["provider"],
                    "source_type": s["source_type"],
                    "timestamp": s.get("timestamp", ""),
                    "snippet": s["snippet"][:200],
                    "metadata": s.get("metadata", {})
                }
                for i, s in enumerate(sources)
            ],
            "total": len(sources)
        })
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ã‰TAPE 10: FIN
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        elapsed_total = round((time.time() - start_time) * 1000)
        
        # Logging pour CI/audit
        logger.info(f"[DEEP_SEARCH] request_id={request_id} user_lang={lang} "
                    f"providers_consulted={len(providers_consulted)} sources_count={len(sources)} "
                    f"confidence_score={confidence['score']} requires_human_review={confidence['requires_human_review']} "
                    f"elapsed_ms={elapsed_total}")
        
        yield sse("complete", {
            "request_id": request_id,
            "elapsed_ms": elapsed_total,
            "sources_count": len(sources),
            "providers_count": len(providers_consulted),
            "providers_target_met": len(providers_consulted) >= MIN_PROVIDERS_TARGET,
            "confidence_score": confidence["score"],
            "requires_human_review": confidence["requires_human_review"]
        })
        
    except Exception as e:
        logger.error(f"Deep search v9 error: {e}", exc_info=True)
        yield sse("error", {"message": str(e), "request_id": request_id})


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENDPOINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.get("/deep")
async def deep_search_endpoint(
    q: str = Query(..., min_length=2, max_length=500, description="RequÃªte de recherche"),
    lang: str = Query("auto", description="Langue (fr, en, auto)")
):
    """Endpoint Deep Search V9."""
    return StreamingResponse(
        deep_search_generator_v9(q, lang),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


# Frontend endpoint - THIS IS WHAT THE FRONTEND CALLS
@router.get("/api/v6/deep-search")
async def deep_search_v6_endpoint(
    q: str = Query(..., min_length=2, max_length=500),
    lang: str = Query("auto")
):
    """Endpoint V6 compatible avec le frontend."""
    return await deep_search_endpoint(q, lang)


# Legacy endpoint for backward compatibility
@router.get("/search")
async def legacy_deep_search(
    q: str = Query(...),
    lang: str = Query("auto")
):
    """Legacy endpoint - redirects to v9."""
    return await deep_search_endpoint(q, lang)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONITORING - TEST TOUTES LES APIs
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.get("/api-status")
async def api_status_check(
    test_query: str = Query("diabetes", description="Query to test APIs with")
):
    """Teste toutes les APIs et retourne statut."""
    import httpx
    from services.api_registry import get_all_enabled_apis
    from urllib.parse import quote
    
    results = []
    
    apis = get_all_enabled_apis()
    
    for api_name, api_config in apis.items():
        api_result = {
            "name": api_name,
            "display_name": api_config.name,
            "category": api_config.category.value,
            "timeout": api_config.timeout,
            "status": "pending",
            "results_count": 0,
            "elapsed_ms": 0,
            "error": None
        }
        
        # Skip internal APIs
        if api_config.url_template.startswith("internal://"):
            api_result["status"] = "internal"
            api_result["error"] = "Internal API - not tested directly"
            results.append(api_result)
            continue
        
        # Skip APIs requiring keys
        if api_config.requires_key:
            import os
            key = os.getenv(api_config.key_env_var, "")
            if not key:
                api_result["status"] = "no_key"
                api_result["error"] = f"Missing {api_config.key_env_var}"
                results.append(api_result)
                continue
        
        start = time.time()
        try:
            url = api_config.url_template.format(query=quote(test_query), lang="en")
            
            async with httpx.AsyncClient(timeout=api_config.timeout) as client:
                resp = await client.get(url, follow_redirects=True)
                
                elapsed_ms = int((time.time() - start) * 1000)
                api_result["elapsed_ms"] = elapsed_ms
                
                if resp.status_code == 200:
                    api_result["status"] = "ok"
                    # Try to count results
                    try:
                        if "json" in resp.headers.get("content-type", ""):
                            data = resp.json()
                            # Try various result counting strategies
                            if isinstance(data, list):
                                api_result["results_count"] = len(data)
                            elif "results" in data:
                                api_result["results_count"] = len(data["results"])
                            elif "items" in data:
                                api_result["results_count"] = len(data["items"])
                            elif "data" in data:
                                api_result["results_count"] = len(data.get("data", []))
                    except:
                        pass
                else:
                    api_result["status"] = "http_error"
                    api_result["error"] = f"HTTP {resp.status_code}"
                    
        except asyncio.TimeoutError:
            api_result["status"] = "timeout"
            api_result["error"] = f"Timeout after {api_config.timeout}s"
            api_result["elapsed_ms"] = int((time.time() - start) * 1000)
        except httpx.ConnectError as e:
            api_result["status"] = "connection_error"
            api_result["error"] = str(e)[:100]
            api_result["elapsed_ms"] = int((time.time() - start) * 1000)
        except Exception as e:
            api_result["status"] = "error"
            api_result["error"] = f"{type(e).__name__}: {str(e)[:100]}"
            api_result["elapsed_ms"] = int((time.time() - start) * 1000)
        
        results.append(api_result)
    
    # Summary
    working = [r for r in results if r["status"] == "ok"]
    failed = [r for r in results if r["status"] not in ["ok", "internal", "no_key", "pending"]]
    
    return {
        "summary": {
            "total_apis": len(results),
            "working": len(working),
            "failed": len(failed),
            "internal": len([r for r in results if r["status"] == "internal"]),
            "no_key": len([r for r in results if r["status"] == "no_key"])
        },
        "working_apis": [r["name"] for r in working],
        "failed_apis": [{"name": r["name"], "error": r["error"]} for r in failed],
        "details": results
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXPORT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Pour import depuis main.py
deep_search_generator = deep_search_generator_v9
