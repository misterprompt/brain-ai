"""
ðŸ“š KNOWLEDGE INTERFACE
======================
Interface Fallback : Wikipedia, SearXNG, DuckDuckGo.
GÃ¨re toutes les requÃªtes gÃ©nÃ©riques non captÃ©es par les autres domaines.
"""

from typing import Dict, List, Any, Optional
from datetime import datetime
from urllib.parse import quote
import re

from .base import BaseInterface


class KnowledgeInterface(BaseInterface):
    """
    Expert GÃ©nÃ©raliste : Connaissances, DÃ©finitions, Recherche Web.
    C'est le FALLBACK utilisÃ© quand aucune autre interface ne match.
    """
    
    DOMAIN_NAME = "knowledge"
    
    # Pas de keywords spÃ©cifiques car c'est le fallback
    KEYWORDS = []
    PATTERNS = []
    
    def matches(self, query: str) -> bool:
        """
        Knowledge match toujours (fallback).
        Mais avec un score trÃ¨s bas pour que les autres aient prioritÃ©.
        """
        return True
    
    def get_match_score(self, query: str) -> int:
        """Score minimum pour Ãªtre le fallback."""
        return 0
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # EXTRACTION DE PARAMÃˆTRES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def extract_params(self, query: str) -> Dict[str, Any]:
        """PrÃ©pare la requÃªte pour la recherche."""
        return {
            "query": query,
            "search_term": quote(query),
            "lang": self._detect_language(query)
        }
    
    def _detect_language(self, query: str) -> str:
        """DÃ©tecte la langue de la requÃªte."""
        q_lower = query.lower()
        
        # FranÃ§ais
        if any(w in q_lower for w in ["qu'est-ce", "comment", "pourquoi", "oÃ¹", "quand", "qui", "quel", "est-ce"]):
            return "fr"
        
        # Anglais par dÃ©faut
        return "en"
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MODE SPEED
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def fetch_speed_data(self, query: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Mode Speed : Wikipedia uniquement (trÃ¨s rapide).
        """
        start = datetime.now()
        
        search_term = params.get("search_term", quote(query))
        # Use language from params (set by user selection), fallback to auto-detect
        lang = params.get("lang", self._detect_language(query))
        
        # Normalize language code (fr-FR -> fr, en-US -> en)
        wiki_lang = lang.split("-")[0] if "-" in str(lang) else lang
        url = f"https://{wiki_lang}.wikipedia.org/api/rest_v1/page/summary/{search_term}"
        
        data = await self._fetch_json(url, timeout=self.SPEED_TIMEOUT)
        
        if not data or "extract" not in data:
            # Fallback DuckDuckGo
            ddg_url = f"https://api.duckduckgo.com/?q={search_term}&format=json&no_html=1"
            ddg_data = await self._fetch_json(ddg_url, timeout=self.SPEED_TIMEOUT)
            
            if ddg_data and ddg_data.get("AbstractText"):
                return self._build_response(
                    success=True,
                    data=ddg_data,
                    context=f"ðŸ“š {query.upper()}:\n{ddg_data.get('AbstractText', '')}",
                    sources=["DuckDuckGo"],
                    start_time=start
                )
            
            return self._build_response(
                success=False,
                data={},
                context="Aucune information trouvÃ©e pour cette recherche.",
                sources=[],
                start_time=start
            )
        
        # Formater la rÃ©ponse Wikipedia
        title = data.get("title", query)
        extract = data.get("extract", "")
        
        context = f"ðŸ“š {title.upper()}:\n{extract}"
        
        return self._build_response(
            success=True,
            data=data,
            context=context,
            sources=[f"Wikipedia {lang.upper()}"],
            start_time=start
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MODE DEEP
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def fetch_deep_data(self, query: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Mode Deep : Wikipedia + DuckDuckGo + Wikidata.
        Uses the language from params (user selection).
        """
        start = datetime.now()
        
        search_term = params.get("search_term", quote(query))
        lang = params.get("lang", "fr")
        
        # Normalize language code
        wiki_lang = lang.split("-")[0] if "-" in str(lang) else lang
        
        urls = [
            # Wikipedia in selected language (primary)
            f"https://{wiki_lang}.wikipedia.org/api/rest_v1/page/summary/{search_term}",
            # DuckDuckGo
            f"https://api.duckduckgo.com/?q={search_term}&format=json&no_html=1",
            # Wikidata in selected language
            f"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={search_term}&language={wiki_lang}&format=json&limit=5",
        ]
        
        results = await self._fetch_multiple(urls, timeout=self.DEEP_TIMEOUT)
        
        sources = []
        aggregated = {}
        
        if results[0] and "extract" in results[0]:
            sources.append(f"Wikipedia {wiki_lang.upper()}")
            aggregated["wikipedia"] = results[0]
        
        if results[1] and (results[1].get("AbstractText") or results[1].get("RelatedTopics")):
            sources.append("DuckDuckGo")
            aggregated["duckduckgo"] = results[1]
        
        if results[2] and results[2].get("search"):
            sources.append("Wikidata")
            aggregated["wikidata"] = results[2]["search"]
        
        # Construire contexte riche
        context_parts = [f"ðŸ“š RECHERCHE: {query.upper()}\n"]
        
        # Wikipedia (prioritaire)
        if aggregated.get("wikipedia"):
            context_parts.append(f"ðŸ“– DÃ‰FINITION ({wiki_lang.upper()}):")
            context_parts.append(f"   {aggregated['wikipedia'].get('extract', '')[:600]}")
        
        # DuckDuckGo - topics associÃ©s
        if aggregated.get("duckduckgo"):
            ddg = aggregated["duckduckgo"]
            if ddg.get("AbstractText"):
                context_parts.append(f"\nðŸ’¡ RÃ‰SUMÃ‰: {ddg['AbstractText'][:300]}")
            
            topics = ddg.get("RelatedTopics", [])[:3]
            if topics:
                context_parts.append("\nðŸ”— SUJETS CONNEXES:")
                for topic in topics:
                    if isinstance(topic, dict) and topic.get("Text"):
                        context_parts.append(f"   â€¢ {topic['Text'][:80]}")
        
        # Wikidata entities
        if aggregated.get("wikidata"):
            context_parts.append("\nðŸ·ï¸ ENTITÃ‰S LIÃ‰ES:")
            for entity in aggregated["wikidata"][:3]:
                label = entity.get("label", "?")
                desc = entity.get("description", "")[:50]
                context_parts.append(f"   â€¢ {label}: {desc}")
        
        return self._build_response(
            success=len(sources) > 0,
            data=aggregated,
            context="\n".join(context_parts),
            sources=sources,
            start_time=start
        )
